---
id: "reddit:1qwjdx6"
source: "reddit"
externalId: "1qwjdx6"
url: "https://www.reddit.com/r/LocalLLaMA/comments/1qwjdx6/is_there_a_good_local_model_to_translate_small/"
title: "Is there a good local model to translate small snippets of text from English to Russian that can be run completely on 12GB VRAM?"
authorHandle: "ShaderCompilation"
authorName: "ShaderCompilation"
publishedAt: "2026-02-05T11:58:59.000Z"
fetchedAt: "2026-02-05T15:01:15.857Z"
tags: []
metrics: {"score":8,"comments":16,"subreddit":"LocalLLaMA"}
score: 5.3580116268063405
scoreBreakdown: {"total":5.3580116268063405,"recency":0.7543453586210133,"engagement":5.199000893386031,"author":0,"source":0.9}
---

Basically the title. I want a model that can be used to translate small snippets of text from books to Russian. But i need it to run on just 12GB of VRAM. Is there a decent model, or 12GB is too small for one?

Edit: I want something that i can run with Ollama
