---
id: "reddit:1qwjdx6"
source: "reddit"
externalId: "1qwjdx6"
url: "https://www.reddit.com/r/LocalLLaMA/comments/1qwjdx6/is_there_a_good_local_model_to_translate_small/"
title: "Is there a good local model to translate small snippets of text from English to Russian that can be run completely on 12GB VRAM?"
text: "Basically the title. I want a model that can be used to translate small snippets of text from books to Russian. But i need it to run on just 12GB of VRAM. Is there a decent model, or 12GB is too small for one?\n\nEdit: I want something that i can run with Ollama"
summary: undefined
image: undefined
imageAlt: undefined
authorHandle: "ShaderCompilation"
authorName: "ShaderCompilation"
publishedAt: "2026-02-05T11:58:59.000Z"
fetchedAt: "2026-02-05T17:01:27.763Z"
tags: []
metrics: {"score":11,"comments":18,"subreddit":"LocalLLaMA"}
score: 5.530942512556254
scoreBreakdown: {"total":5.530942512556254,"recency":0.7258102653470131,"engagement":5.419681415271047,"author":0,"source":0.9}
---

Basically the title. I want a model that can be used to translate small snippets of text from books to Russian. But i need it to run on just 12GB of VRAM. Is there a decent model, or 12GB is too small for one?

Edit: I want something that i can run with Ollama
