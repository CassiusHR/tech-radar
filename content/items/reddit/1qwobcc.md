---
id: "reddit:1qwobcc"
source: "reddit"
externalId: "1qwobcc"
url: "https://www.reddit.com/r/LocalLLaMA/comments/1qwobcc/strix_halo_benchmarks_13_models_15_llamacpp_builds/"
title: "Strix Halo benchmarks: 13 models, 15 llama.cpp builds"
text: "https://preview.redd.it/feayylk82phg1.png?width=3469&amp;format=png&amp;auto=webp&amp;s=fd82806fb3743ba1b57c2ade12ef4d71e25679bf\n\nRan a software ablation study on the Strix Halo's iGPU testing anything I could fine (ROCm, Vulkan, gfx version, hipblaslt on/off, rocWMMA, various Vulkan/RADV options) across different build configurations. Rather than fighting dependency hell to find \"the\" working setup, I dockerized 15 different llama.cpp builds and let them all run. Some failed but that's ok, that"
summary: undefined
image: undefined
imageAlt: undefined
authorHandle: "Beneficial-Shame-483"
authorName: "Beneficial-Shame-483"
publishedAt: "2026-02-05T15:30:19.000Z"
fetchedAt: "2026-02-05T17:01:27.763Z"
tags: ["pillar/devops"]
metrics: {"score":27,"comments":18,"subreddit":"LocalLLaMA"}
score: 5.939258784291895
scoreBreakdown: {"total":5.939258784291895,"recency":0.7767401102874547,"engagement":5.82243631670354,"author":0,"source":0.9}
---

https://preview.redd.it/feayylk82phg1.png?width=3469&amp;format=png&amp;auto=webp&amp;s=fd82806fb3743ba1b57c2ade12ef4d71e25679bf

Ran a software ablation study on the Strix Halo's iGPU testing anything I could fine (ROCm, Vulkan, gfx version, hipblaslt on/off, rocWMMA, various Vulkan/RADV options) across different build configurations. Rather than fighting dependency hell to find "the" working setup, I dockerized 15 different llama.cpp builds and let them all run. Some failed but that's ok, that
