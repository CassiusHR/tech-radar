---
id: "reddit:1qwbmct"
source: "reddit"
externalId: "1qwbmct"
url: "https://www.reddit.com/r/LocalLLaMA/comments/1qwbmct/qwen3codernext_on_rtx_5060_ti_16_gb_some_numbers/"
title: "Qwen3-Coder-Next on RTX 5060 Ti 16 GB - Some numbers"
text: "About 2 weeks ago, I posted about running GLM-4.7-Flash on 16 GB of VRAM here www.reddit.com/r/LocalLLaMA/comments/1qlanzn/glm47flashreap_on_rtx_5060_ti_16_gb_200k_context/. \nAnd here we go, today, let's squeeze an even bigger model into the poor rig.\n\nHardware:\n- AMD Ryzen 7 7700X\n- RAM 32 GB DDR5-6000\n- RTX 5060 Ti 16 GB\n\nModel: [unsloth/Qwen3-Coder-Next-GGUF Q3_K_M](https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF?show_file_info=Qwen3-Coder-Next-Q3_K_M.gguf)\n\nLlama.cpp version: [llama.cpp"
summary: undefined
image: undefined
imageAlt: undefined
authorHandle: "bobaburger"
authorName: "bobaburger"
publishedAt: "2026-02-05T04:33:49.000Z"
fetchedAt: "2026-02-05T17:01:27.763Z"
tags: ["pillar/huggingface"]
metrics: {"score":192,"comments":77,"subreddit":"LocalLLaMA"}
score: 7.936420631285574
scoreBreakdown: {"total":7.936420631285574,"recency":0.6291904539472585,"engagement":8.189054691925602,"author":0,"source":0.9}
---

About 2 weeks ago, I posted about running GLM-4.7-Flash on 16 GB of VRAM here www.reddit.com/r/LocalLLaMA/comments/1qlanzn/glm47flashreap_on_rtx_5060_ti_16_gb_200k_context/. 
And here we go, today, let's squeeze an even bigger model into the poor rig.

Hardware:
- AMD Ryzen 7 7700X
- RAM 32 GB DDR5-6000
- RTX 5060 Ti 16 GB

Model: [unsloth/Qwen3-Coder-Next-GGUF Q3_K_M](https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF?show_file_info=Qwen3-Coder-Next-Q3_K_M.gguf)

Llama.cpp version: [llama.cpp
