---
id: "reddit:1qwbmct"
source: "reddit"
externalId: "1qwbmct"
url: "https://www.reddit.com/r/LocalLLaMA/comments/1qwbmct/qwen3codernext_on_rtx_5060_ti_16_gb_some_numbers/"
title: "Qwen3-Coder-Next on RTX 5060 Ti 16 GB - Some numbers"
authorHandle: "bobaburger"
authorName: "bobaburger"
publishedAt: "2026-02-05T04:33:49.000Z"
fetchedAt: "2026-02-05T15:01:15.857Z"
tags: ["pillar/huggingface"]
metrics: {"score":176,"comments":72,"subreddit":"LocalLLaMA"}
score: 7.860550076996892
scoreBreakdown: {"total":7.860550076996892,"recency":0.6539269576145244,"engagement":8.080017572382022,"author":0,"source":0.9}
---

About 2 weeks ago, I posted about running GLM-4.7-Flash on 16 GB of VRAM here www.reddit.com/r/LocalLLaMA/comments/1qlanzn/glm47flashreap_on_rtx_5060_ti_16_gb_200k_context/. 
And here we go, today, let's squeeze an even bigger model into the poor rig.

Hardware:
- AMD Ryzen 7 7700X
- RAM 32 GB DDR5-6000
- RTX 5060 Ti 16 GB

Model: [unsloth/Qwen3-Coder-Next-GGUF Q3_K_M](https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF?show_file_info=Qwen3-Coder-Next-Q3_K_M.gguf)

Llama.cpp version: [llama.cpp
