---
id: "reddit:1qvs003"
source: "reddit"
externalId: "1qvs003"
url: "https://www.reddit.com/r/MachineLearning/comments/1qvs003/d_using_sort_as_an_activation_function_fixes/"
title: "[D] Using SORT as an activation function fixes spectral bias in MLPs"
authorHandle: "kiockete"
authorName: "kiockete"
publishedAt: "2026-02-04T15:45:40.000Z"
fetchedAt: "2026-02-05T14:45:09.329Z"
tags: []
metrics: {"score":47,"comments":23,"subreddit":"MachineLearning"}
score: 6.186905989807273
scoreBreakdown: {"total":6.186905989807273,"recency":0.5137272934967424,"engagement":6.360612695178005,"author":0,"source":0.9}
---

[SortDC vs. SIREN vs. ReLU on image compression task](https://preview.redd.it/zn55f2vlrhhg1.png?width=1837&amp;format=png&amp;auto=webp&amp;s=4aa4fb3e1e872fe182b2f17e103ed7d015493cd1)

Training an INR with standard MLPs (ReLU/SiLU) results in blurry images unless we use Fourier Features or periodic activations (like SIREN), but it turns out you can just sort the feature vector before passing it to the next layer and it somehow fixes the spectral bias of MLPs. Instead of ReLU the activation funct
