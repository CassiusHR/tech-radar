---
id: "reddit:1qwgyrn"
source: "reddit"
externalId: "1qwgyrn"
url: "https://www.reddit.com/r/LocalLLaMA/comments/1qwgyrn/best_deep_research_for_local_llm_in_2026/"
title: "Best \"Deep research\" for local LLM in 2026 - platforms/tools/interface/setups"
authorHandle: "liviuberechet"
authorName: "liviuberechet"
publishedAt: "2026-02-05T09:39:18.000Z"
fetchedAt: "2026-02-05T14:45:34.462Z"
tags: ["pillar/anthropic-openai-releases"]
metrics: {"score":39,"comments":25,"subreddit":"LocalLLaMA"}
score: 6.322185775888607
scoreBreakdown: {"total":6.322185775888607,"recency":0.7249173236360807,"engagement":6.2997335384623705,"author":0,"source":0.9}
---

I've been using the **Deep research** function from ChatGPT quite a lot since it came out.

I love it, but every month I use the limit in the first 2-3 days... so I was wondering if anyone else has any tips or setups they use for running something similar to Deep research -- on local LLM.

I have a decent setup of 3x3090, so I can run big-ish models (gpt-oss-120b or GLM Air) at VRAM speed or 30b models in Q8 (if precision is more important for deep research).

I've been using OpenWebUI + local S
