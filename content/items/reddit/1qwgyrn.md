---
id: "reddit:1qwgyrn"
source: "reddit"
externalId: "1qwgyrn"
url: "https://www.reddit.com/r/LocalLLaMA/comments/1qwgyrn/best_deep_research_for_local_llm_in_2026/"
title: "Best \"Deep research\" for local LLM in 2026 - platforms/tools/interface/setups"
text: "I've been using the **Deep research** function from ChatGPT quite a lot since it came out.\n\nI love it, but every month I use the limit in the first 2-3 days... so I was wondering if anyone else has any tips or setups they use for running something similar to Deep research -- on local LLM.\n\nI have a decent setup of 3x3090, so I can run big-ish models (gpt-oss-120b or GLM Air) at VRAM speed or 30b models in Q8 (if precision is more important for deep research).\n\nI've been using OpenWebUI + local S"
summary: undefined
image: undefined
imageAlt: undefined
authorHandle: "liviuberechet"
authorName: "liviuberechet"
publishedAt: "2026-02-05T09:39:18.000Z"
fetchedAt: "2026-02-05T17:01:27.763Z"
tags: ["pillar/anthropic-openai-releases"]
metrics: {"score":69,"comments":31,"subreddit":"LocalLLaMA"}
score: 6.776925482090412
scoreBreakdown: {"total":6.776925482090412,"recency":0.6939945107017617,"engagement":6.835922691620919,"author":0,"source":0.9}
---

I've been using the **Deep research** function from ChatGPT quite a lot since it came out.

I love it, but every month I use the limit in the first 2-3 days... so I was wondering if anyone else has any tips or setups they use for running something similar to Deep research -- on local LLM.

I have a decent setup of 3x3090, so I can run big-ish models (gpt-oss-120b or GLM Air) at VRAM speed or 30b models in Q8 (if precision is more important for deep research).

I've been using OpenWebUI + local S
